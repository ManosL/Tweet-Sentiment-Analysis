{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Data Mining Techniques </h1>\n",
    "    <h1> Project 1 </h1>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left>\n",
    "    <h2> Authors </h2>\n",
    "    <ul>\n",
    "        <li> \n",
    "            <b>Name:</b> Emmanouil Lykos <br />\n",
    "            <b>ID Number:</b> 1115201600096 <br />\n",
    "        </li>\n",
    "        <br />\n",
    "        <li> \n",
    "            <b>Name:</b> Georgios Liakopoulos <br />\n",
    "            <b>ID Number:</b> 1115201600091 <br />\n",
    "        </li>\n",
    "      </ul>\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = 'left'> Project Report </h2> <br />\n",
    "Just importing the necessary libraries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from pickle import load,dump\n",
    "\n",
    "# Used to manipulate the data sets\n",
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd \n",
    "\n",
    "# Used for cleaning the dataset from unnecessary words\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib #only needed to determine Matplotlib version number\n",
    "import wordcloud\n",
    "from sklearn.manifold import TSNE  # Used for dimensionality reduction\n",
    "                                   # at tsne_plot()\n",
    "    \n",
    "# Vectorization Libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "\n",
    "######################\n",
    "\n",
    "# Classification Libarries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "##########################\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reading the dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readLexica():\n",
    "    # These variables are holding the words\n",
    "    # that we read from that dictionary\n",
    "    \n",
    "    affin_words = set()\n",
    "    valence_tweet_words = set()\n",
    "    generic_words = set()\n",
    "    nrc_val_words = set()\n",
    "    nrctag_val_words = set()\n",
    "    \n",
    "    # Dictionary with format phrase:valence\n",
    "    \n",
    "    affin_weights = {}\n",
    "    valence_tweet_weights = {}\n",
    "    generic_weights = {}\n",
    "    nrc_val_weights = {}\n",
    "    nrctag_val_weights = {}\n",
    "    \n",
    "    # The paths of the dictionaries\n",
    "    \n",
    "    filePaths = ['../lexica/affin/affin.txt','../lexica/emotweet/valence_tweet.txt',\n",
    "    '../lexica/generic/generic.txt','../lexica/nrc/val.txt','../lexica/nrctag/val.txt']\n",
    "\n",
    "    sets = [affin_words,valence_tweet_words,generic_words,\n",
    "            nrc_val_words,nrctag_val_words]\n",
    "    \n",
    "    weights = [affin_weights,valence_tweet_weights,generic_weights,\n",
    "            nrc_val_weights,nrctag_val_weights]\n",
    "\n",
    "    for fileName,set_,weight_dict in zip(filePaths,sets,weights):\n",
    "        file = open(fileName,'r')\n",
    "\n",
    "        fileLines = file.readlines()\n",
    "        \n",
    "        for line in fileLines:\n",
    "            # Retrieving the tokens of current line\n",
    "            linelist = line.split(' ')\n",
    "            linelist2 = []\n",
    "\n",
    "            for token in linelist:\n",
    "                linelist2 += token.split('\\t')\n",
    "            \n",
    "            linelist = linelist2\n",
    "            \n",
    "            # Joining together all the tokens except\n",
    "            # the last one, because the last token\n",
    "            # represents the valence of the word/phrase\n",
    "            \n",
    "            word = ' '.join(linelist[0:len(linelist)-1])\n",
    "\n",
    "            if word not in set_:\n",
    "                set_.add(word)\n",
    "                weight_dict[word] = float(linelist[len(linelist) - 1])\n",
    "\n",
    "            file.close()\n",
    "    \n",
    "    return tuple(weights)\n",
    "\n",
    "# Retieving the dictionaries\n",
    "affin_dict,valence_tweet_dict,generic_dict,nrc_val_dict,nrctag_val_dict = readLexica()\n",
    "dictionaries = [affin_dict,valence_tweet_dict,nrc_val_dict,nrctag_val_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training dataset, the test dataset and test gold to get the necessary information\n",
    "\n",
    "**Note**: We read the tsv files with the hard-coded way because when we called the read_csv function\n",
    "with separator equal with '\\t', not all tweets from the training set file was read. Thus, we prefered\n",
    "the safe way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit, watch Rafa and Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I'm a GSP fan, i just hate Nick D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran, Mon Amour: Obama Tried to Establish Ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I sat through this whole movie just for Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT's &amp;&amp; SAT's, deciding where I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why is \\\"\"Happy Valentines Day\\\"\" trending? It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas, but Dalla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Im bringing the monster load of candy tomorrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Apple software, retail chiefs out in overhaul:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>positive</td>\n",
       "      <td>@oluoch @victor_otti @kunjand I just watched i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive</td>\n",
       "      <td>One of my best 8th graders Kory was excited af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Livewire Nadal confirmed for Mexican Open in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>positive</td>\n",
       "      <td>@MsSheLahY I didnt want to just pop up... but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Alyoup005 @addicted2haley hmmmm  November is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>neutral</td>\n",
       "      <td>#Iran US delisting MKO from global terrorists ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>positive</td>\n",
       "      <td>@JackStirling serge is amazing... like hes act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@HatersgonnHate_ @HAMlikeHussain @Ramythe3lite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>positive</td>\n",
       "      <td>Good Morning Becky ! Thursday is going to be F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Expect light-moderate rains over E. Visayas; C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>positive</td>\n",
       "      <td>One ticket left for the @49ers game tomorrow! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>negative</td>\n",
       "      <td>AFC away fans on Saturday. All this stuff abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>negative</td>\n",
       "      <td>My Saturday night has consisted of me watching...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>negative</td>\n",
       "      <td>Why is it so hard to find the @TVGuideMagazine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Game 1 of the NLCS and a rematch of the NFC Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>neutral</td>\n",
       "      <td>James hall LIVE in Indianapolis Dec.8th.. CHRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@TrevorJavier the heat game may cost alot more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>positive</td>\n",
       "      <td>Never start working on your dreams and goals t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28031</th>\n",
       "      <td>positive</td>\n",
       "      <td>Don't miss How to Bake a Man with @jessicainla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28032</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@My_Bird_Tweets how you feeling about the Colt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28033</th>\n",
       "      <td>positive</td>\n",
       "      <td>Sangam University Bhilwara Salutes Nobel Laure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28034</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@vinaykumargupta Modi will come and go...dynas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28035</th>\n",
       "      <td>positive</td>\n",
       "      <td>The FIA ran the race the right way - Lauda: Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28036</th>\n",
       "      <td>positive</td>\n",
       "      <td>15 Experts Reveal the Best Social Media Apps f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28037</th>\n",
       "      <td>positive</td>\n",
       "      <td>Woo Hoo!  It's Friday!!  Stephen Colbert Sings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28038</th>\n",
       "      <td>neutral</td>\n",
       "      <td>If you coming to my house Saturday in the morn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28039</th>\n",
       "      <td>positive</td>\n",
       "      <td>Me &amp;amp; Nia was having the best convo about r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28040</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Guess I'm going to Knott's Scary Farm on Wedne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28041</th>\n",
       "      <td>negative</td>\n",
       "      <td>@johnWKYC Green Bay drafted Rogers at about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28042</th>\n",
       "      <td>negative</td>\n",
       "      <td>@joseleatleti @Dave_llb aaah ...1st fixture li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28043</th>\n",
       "      <td>neutral</td>\n",
       "      <td>ABC: Giant pandas may be cute, but it takes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28044</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Condole the passing of Lee Soo Man's wife from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28045</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Alonso is still a Ferrari driver today but he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28046</th>\n",
       "      <td>neutral</td>\n",
       "      <td>heading to West Wylong tomorrow for a couple o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28047</th>\n",
       "      <td>neutral</td>\n",
       "      <td>October 17 - AirBridgeCargo Airlines (ABC) has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28048</th>\n",
       "      <td>positive</td>\n",
       "      <td>Yo @annietudzin you excited for this packers g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28049</th>\n",
       "      <td>positive</td>\n",
       "      <td>So Knott's tomorrow lmao ima have my bro drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28050</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Niki Lauda just confirmed to Sky that Alonso w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28051</th>\n",
       "      <td>positive</td>\n",
       "      <td>@ImTommyHill @brianlawless I notice Niki Lauda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28052</th>\n",
       "      <td>positive</td>\n",
       "      <td>Deep Condolance for lee soo man's wife. May sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28053</th>\n",
       "      <td>positive</td>\n",
       "      <td>Just means two of Atleti, Real or Barca will b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28054</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Finishing up the FEC pprwrk for 3rd quarter. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28055</th>\n",
       "      <td>positive</td>\n",
       "      <td>01 Oct 2014 Rock on Dandiya Nite at Sangam Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28056</th>\n",
       "      <td>neutral</td>\n",
       "      <td>the day after newark ill be able to say \"\"i me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>neutral</td>\n",
       "      <td>FEC hold farewell session for seven ministers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28058</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Luca Di Montezemolo (who's last day was Monday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28059</th>\n",
       "      <td>positive</td>\n",
       "      <td>Coffee is pretty much the answer to all questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28060</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Niki Lauda just confirmed to Sky that Alonso w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28061 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tag                                              Tweet\n",
       "0      positive  Gas by my house hit $3.39!!!! I'm going to Cha...\n",
       "1      negative  Theo Walcott is still shit, watch Rafa and Joh...\n",
       "2      negative  its not that I'm a GSP fan, i just hate Nick D...\n",
       "3      negative  Iranian general says Israel's Iron Dome can't ...\n",
       "4       neutral  Tehran, Mon Amour: Obama Tried to Establish Ti...\n",
       "5       neutral  I sat through this whole movie just for Harry ...\n",
       "6      positive  with J Davlar 11th. Main rivals are team Polan...\n",
       "7      negative  Talking about ACT's && SAT's, deciding where I...\n",
       "8       neutral  Why is \\\"\"Happy Valentines Day\\\"\" trending? It...\n",
       "9      negative  They may have a SuperBowl in Dallas, but Dalla...\n",
       "10      neutral  Im bringing the monster load of candy tomorrow...\n",
       "11      neutral  Apple software, retail chiefs out in overhaul:...\n",
       "12     positive  @oluoch @victor_otti @kunjand I just watched i...\n",
       "13     positive  One of my best 8th graders Kory was excited af...\n",
       "14      neutral  #Livewire Nadal confirmed for Mexican Open in ...\n",
       "15     positive  @MsSheLahY I didnt want to just pop up... but ...\n",
       "16      neutral  @Alyoup005 @addicted2haley hmmmm  November is ...\n",
       "17      neutral  #Iran US delisting MKO from global terrorists ...\n",
       "18     positive  @JackStirling serge is amazing... like hes act...\n",
       "19      neutral  @HatersgonnHate_ @HAMlikeHussain @Ramythe3lite...\n",
       "20     positive  Good Morning Becky ! Thursday is going to be F...\n",
       "21      neutral  Expect light-moderate rains over E. Visayas; C...\n",
       "22     positive  One ticket left for the @49ers game tomorrow! ...\n",
       "23     negative  AFC away fans on Saturday. All this stuff abou...\n",
       "24     negative  My Saturday night has consisted of me watching...\n",
       "25     negative  Why is it so hard to find the @TVGuideMagazine...\n",
       "26      neutral  Game 1 of the NLCS and a rematch of the NFC Ch...\n",
       "27      neutral  James hall LIVE in Indianapolis Dec.8th.. CHRI...\n",
       "28      neutral  @TrevorJavier the heat game may cost alot more...\n",
       "29     positive  Never start working on your dreams and goals t...\n",
       "...         ...                                                ...\n",
       "28031  positive  Don't miss How to Bake a Man with @jessicainla...\n",
       "28032   neutral  @My_Bird_Tweets how you feeling about the Colt...\n",
       "28033  positive  Sangam University Bhilwara Salutes Nobel Laure...\n",
       "28034   neutral  @vinaykumargupta Modi will come and go...dynas...\n",
       "28035  positive  The FIA ran the race the right way - Lauda: Ni...\n",
       "28036  positive  15 Experts Reveal the Best Social Media Apps f...\n",
       "28037  positive  Woo Hoo!  It's Friday!!  Stephen Colbert Sings...\n",
       "28038   neutral  If you coming to my house Saturday in the morn...\n",
       "28039  positive  Me &amp; Nia was having the best convo about r...\n",
       "28040   neutral  Guess I'm going to Knott's Scary Farm on Wedne...\n",
       "28041  negative  @johnWKYC Green Bay drafted Rogers at about th...\n",
       "28042  negative  @joseleatleti @Dave_llb aaah ...1st fixture li...\n",
       "28043   neutral  ABC: Giant pandas may be cute, but it takes a ...\n",
       "28044   neutral  Condole the passing of Lee Soo Man's wife from...\n",
       "28045   neutral  Alonso is still a Ferrari driver today but he ...\n",
       "28046   neutral  heading to West Wylong tomorrow for a couple o...\n",
       "28047   neutral  October 17 - AirBridgeCargo Airlines (ABC) has...\n",
       "28048  positive  Yo @annietudzin you excited for this packers g...\n",
       "28049  positive  So Knott's tomorrow lmao ima have my bro drive...\n",
       "28050   neutral  Niki Lauda just confirmed to Sky that Alonso w...\n",
       "28051  positive  @ImTommyHill @brianlawless I notice Niki Lauda...\n",
       "28052  positive  Deep Condolance for lee soo man's wife. May sh...\n",
       "28053  positive  Just means two of Atleti, Real or Barca will b...\n",
       "28054   neutral  Finishing up the FEC pprwrk for 3rd quarter. T...\n",
       "28055  positive  01 Oct 2014 Rock on Dandiya Nite at Sangam Uni...\n",
       "28056   neutral  the day after newark ill be able to say \"\"i me...\n",
       "28057   neutral  FEC hold farewell session for seven ministers ...\n",
       "28058   neutral  Luca Di Montezemolo (who's last day was Monday...\n",
       "28059  positive  Coffee is pretty much the answer to all questi...\n",
       "28060   neutral  Niki Lauda just confirmed to Sky that Alonso w...\n",
       "\n",
       "[28061 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the necessary paths\n",
    "train_set_path = '../twitter_data/train2017.tsv'\n",
    "test_set_path  = '../twitter_data/test2017.tsv'\n",
    "test_set_gold_path  = '../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "\n",
    "filesToRead = [train_set_path,test_set_path,test_set_gold_path]\n",
    "dataframes  = []\n",
    "\n",
    "for filePath in filesToRead:\n",
    "    file = open(filePath,\"r\")\n",
    "\n",
    "    line = file.readline()\n",
    "    lines = []\n",
    "\n",
    "    while len(line) != 0:\n",
    "        line = line.split(\"\\t\")\n",
    "\n",
    "        line[len(line) - 1] = line[len(line) - 1].replace('\\n','')\n",
    "\n",
    "        if len(line) > 0:\n",
    "            lines.append(line)\n",
    "\n",
    "        line = file.readline()\n",
    "\n",
    "    if filePath != test_set_gold_path:\n",
    "        curr_df = pd.DataFrame(data = lines,columns = ['ID1','ID2','Tag','Tweet'])\n",
    "        curr_df = curr_df[['Tag','Tweet']]     # Getting the columns that we care for\n",
    "        dataframes.append(curr_df)\n",
    "    else:\n",
    "        curr_df = pd.DataFrame(data = lines,columns = ['ID1','Tag'])   \n",
    "        curr_df = curr_df[['Tag']]        # We suppose that original tags are written with the\n",
    "                                          # same sequence as tweets at file\n",
    "        dataframes.append(curr_df)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "training_dataframe = dataframes[0]\n",
    "test_dataframe = dataframes[1]\n",
    "test_solutions = dataframes[2]\n",
    "\n",
    "#Example\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the **sentimel tendency** of the tweets of **training** set and printing a bar graph\n",
    "\n",
    "**Note**: We did that, in this point of the program because clean-up takes time and it's not a graph\n",
    "that will change after the clean-up, so it is better to be there as an initial statistic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweets Percentage 41.0 percent\n",
      "Negative Tweets Percentage 16.0 percent\n",
      "Neutral Tweets Percentage 43.0 percent\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting a series object which has the number\n",
    "# of occurences of different value of Tag column\n",
    "\n",
    "tags = training_dataframe['Tag'].value_counts()\n",
    "\n",
    "neutral_num = tags['neutral']\n",
    "positive_num = tags['positive']\n",
    "negative_num = tags['negative']\n",
    "\n",
    "print(\"Positive Tweets Percentage %.1f percent\" % ((float(positive_num) / len(training_dataframe))*100))\n",
    "print(\"Negative Tweets Percentage %.1f percent\" % ((float(negative_num) / len(training_dataframe))*100))\n",
    "print(\"Neutral Tweets Percentage %.1f percent\" % ((float(neutral_num) / len(training_dataframe))*100))\n",
    "\n",
    "# Printing the bar plot for tweets\n",
    "\n",
    "tags.plot.bar(title = 'Tweets sentimel tendency')\n",
    "plt.xticks(rotation = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can conclude that in the training dataset the tweets have\n",
    "**neutral** and **positive** inclination, and not **negative** one. Thus, probably\n",
    "someone must have come to the conclusion that at Twitter most post are **neutral** and\n",
    "**positive**, because we could see from each **sentimel** percentage that the number\n",
    "of **positive** tweets are close to the number of **neutral** tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Clean-Up Part</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the program that clean the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list to remove the words such as 1st or 5th , the numbers has been removed earlier\n",
    "# so the word will be st or th\n",
    "more_stopwords = ['st','th']\n",
    "\n",
    "#function to predict if a word is adjective noun verb etc , for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#function to remove the stopwords from a tweet\n",
    "def stop_words(stemmer,tweet):\n",
    "\n",
    "    tweet = [ lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tweet if (word not in stopwords.words('english') \n",
    "                                                    and len(word) > 1 and word not in more_stopwords) or word == 'not']\n",
    "                                                     \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# CLEANUP PHASE #\n",
    "\n",
    "#make every letter of the tweets in lower case\n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: t.lower())\n",
    "\n",
    "#remove the tags \n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: re.sub(\"@[a-z!\\\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~+]+\",\"\",t))\n",
    "\n",
    "#remove the hashtags\n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: re.sub(\"#[a-z!\\\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~+]+\",\"\",t))\n",
    "\n",
    "#remove the links\n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: re.sub(\"http[a-z!\\\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~]+\",\"\",t))\n",
    "\n",
    "#remove every character which is not letter and space\n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: re.sub(r'[^a-z ]',\" \",t))\n",
    "\n",
    "#tokenize the tweets word by word\n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: nltk.word_tokenize(t) )\n",
    "\n",
    "#do lemmatization \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "training_dataframe['Tweet'] = training_dataframe.Tweet.apply(lambda t: ' '.join( stop_words(lemmatizer,t) ))\n",
    "\n",
    "\n",
    "#do the same as above in the test set\n",
    "\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: t.lower())\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: re.sub(\"@[a-z!\\\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~+]+\",\"\",t))\n",
    "\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: re.sub(\"#[a-z!\\\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~+]+\",\"\",t))\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: re.sub(\"http[a-z!\\\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~]+\",\"\",t))\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: re.sub(r'[^a-z ]',\" \",t))\n",
    "\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: nltk.word_tokenize(t) )\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "test_dataframe['Tweet'] = test_dataframe.Tweet.apply(lambda t: ' '.join( stop_words(lemmatizer,t) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics Part</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will check if the tweets of each category have any critical difference at their cleaned-up tweet length. We did this analysis on the cleaned tweets for two reasons:\n",
    "\n",
    "<ul>\n",
    "    <li>When we did that analysis on the initial dataset, without cleaning the tweets, we saw that the average length and the standard deviation of the lengths was the same at the lengths of the tweets of each category.</li>\n",
    "    <li>The cleaned tweets contain more essential words that the uncleared tweets which contain stopwords or generally words that don't change the sentimel of the tweet.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of tokens/words of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe['WordCount'] = training_dataframe.Tweet.apply(lambda t: len(t.split()))\n",
    "\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting **histograns** for each occuring tweet length of **each** category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for attr in ['positive','negative','neutral']:\n",
    "    wanted_tweets = training_dataframe[training_dataframe['Tag'] == attr] # Getting the tweets of the current class\n",
    "    wanted_tweets_count = wanted_tweets['WordCount']                      # Getting their WordCount column\n",
    "    \n",
    "    # Getting maximum/minimum/average length and standard deviation\n",
    "    \n",
    "    lines.append([max(wanted_tweets_count),min(wanted_tweets_count),\n",
    "                  sum(wanted_tweets_count)/len(wanted_tweets_count),\n",
    "                  np.std(wanted_tweets_count)])\n",
    "    \n",
    "    wanted_tweets = wanted_tweets.sort_values(by = 'WordCount')\n",
    "    hist = wanted_tweets['WordCount'].value_counts(sort = False).sort_index()\n",
    "    hist.plot.bar(title = attr.title() + \" tweet no. of words distribution\")  # Printing the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data = lines,columns = ['Max','Min','Average','Standard Deviation'],\n",
    "                   index = ['positive','negative','neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs and the table above, I can conclude that the tweet length of each category follows something like a normal distribution and the normal distributions of each category have very close average and standard deviation, one another. Consequently, we don't think that taking as an extra feature the length of the tweet will increase our accuracy because the tweets of each category do not have a significant length difference with one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create word clouds with the most common adjectives and verbs of our all, positive, negative and neutral tweets. We chose to take the adjectives and verbs because we think that these words are the most critical to determine the sentimel of each tweet. Furthermore, when doing the following procedure to collect these words when we get a word that is on a negative or positive tweet we check if the words is at a neutral tweet, and if there is we do not include it on a the list of positive/negative list of adjectives and verbs, because if we do them we will not condlude anything by the word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs_and_verbs = []\n",
    "all_adjs_and_verbs_pos = []\n",
    "\n",
    "all_adjs_and_verbs_neutral = []\n",
    "neutral_adjs_and_verbs = set()\n",
    "\n",
    "all_adjs_and_verbs_neg = []\n",
    "\n",
    "# Collecting the necessary words\n",
    "for tweet,tweet_tag in zip(training_dataframe['Tweet'],training_dataframe['Tag']):\n",
    "    splitted_tweet = tweet.split(' ')\n",
    "    splitted_tweet = [word for word in splitted_tweet if len(word) > 1]\n",
    "\n",
    "    pos_tags = pos_tag(splitted_tweet)\n",
    "    \n",
    "    # The word not is like to belong in every class\n",
    "    for tag in pos_tags:\n",
    "        if tag[0] == 'not' or ((tag[1][0] == 'J' or tag[1][0] == 'V') and len(tag[0]) > 1):\n",
    "            all_adjs_and_verbs.append(tag[0])\n",
    "\n",
    "            if tweet_tag == 'positive':\n",
    "                if tag[0] == 'not' or (tag[0] not in neutral_adjs_and_verbs):\n",
    "                    all_adjs_and_verbs_pos.append(tag[0])\n",
    "            elif tweet_tag == 'negative':\n",
    "                if tag[0] == 'not' or (tag[0] not in neutral_adjs_and_verbs):\n",
    "                    all_adjs_and_verbs_neg.append(tag[0])\n",
    "            else:\n",
    "                all_adjs_and_verbs_neutral.append(tag[0])\n",
    "                neutral_adjs_and_verbs.add(tag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the **word clouds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>All Words<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs_and_verbs_text = ' '.join(all_adjs_and_verbs)\n",
    "\n",
    "cloud = wordcloud.WordCloud().generate(all_adjs_and_verbs_text)\n",
    "\n",
    "plt.title(\"All Words\")\n",
    "plt.imshow(cloud,interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Positive Words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs_and_verbs_text = ' '.join(all_adjs_and_verbs_pos)\n",
    "\n",
    "cloud = wordcloud.WordCloud().generate(all_adjs_and_verbs_text)\n",
    "\n",
    "plt.title(\"Positive Words\")\n",
    "plt.imshow(cloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Negative Words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs_and_verbs_text = ' '.join(all_adjs_and_verbs_neg)\n",
    "\n",
    "cloud = wordcloud.WordCloud().generate(all_adjs_and_verbs_text)\n",
    "\n",
    "plt.title(\"Negative Words\")\n",
    "plt.imshow(cloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Neutral Words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adjs_and_verbs_text = ' '.join(all_adjs_and_verbs_neutral)\n",
    "cloud = wordcloud.WordCloud().generate(all_adjs_and_verbs_text)\n",
    "\n",
    "plt.title(\"Neutral Words\")\n",
    "plt.imshow(cloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above wordclouds that the most common words that appear at neutral tweets are kind of identical with the most common words of all tweets, hence, we think it was wise that we deleted the words that appear at neutral tweets from the words that appear on positive or negative tweets because if we didn't probably we would have identical word clouds because the neutral words are the most cmmon thus they will obfuscate the words with true positive or negative meaning.\n",
    "\n",
    "Moreover, we can see at positive and negative wordclouds that they have as most common words, words that as units have positive and negative meaning, respectively. Consequntly, we can assume at this point that we did a good cleaning on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Vectorization Part</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bag of Words Vectorization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Simple, just calling the necessary function\n",
    "# for training and testing set\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_features = 1000,stop_words = 'english')\n",
    "\n",
    "bow_xtrain = bow_vectorizer.fit_transform(training_dataframe['Tweet'])\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_features = 1000,stop_words = 'english')\n",
    "\n",
    "bow_xtest  = bow_vectorizer.fit_transform(test_dataframe['Tweet'])\n",
    "\n",
    "bow_xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>TF-IDF Vectorization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 1000,stop_words = 'english')\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(training_dataframe['Tweet'])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 1000,stop_words = 'english')\n",
    "\n",
    "tfidf_test  = tfidf_vectorizer.fit_transform(test_dataframe['Tweet'])\n",
    "\n",
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Word2Vec Vectorization</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the Word2Vec models of training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if already exists a pickle file for each model\n",
    "# because the training takes too much time\n",
    "\n",
    "if not os.path.isfile('./pickle_files/train_w2v_model.pkl'):\n",
    "    tokenized_tweet = training_dataframe['Tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "    model_w2v_train = gensim.models.Word2Vec(\n",
    "                tokenized_tweet,\n",
    "                size=500, # desired no. of features/independent variables\n",
    "                window=5, # context window size\n",
    "                min_count=2,\n",
    "                sg = 1, # 1 for skip-gram model\n",
    "                hs = 0,\n",
    "                negative = 10, # for negative sampling\n",
    "                workers= 2, # no.of cores\n",
    "                seed = 34)\n",
    "    \n",
    "    #Training the model\n",
    "    model_w2v_train.train(tokenized_tweet, total_examples= len(training_dataframe['Tweet']),\n",
    "                         epochs=20)\n",
    "    \n",
    "    # Writing the model to a pickle file\n",
    "    dump(model_w2v_train,open(\"./pickle_files/train_w2v_model.pkl\",\"w+b\"))\n",
    "else:\n",
    "    # If there already exists a pickle file I just load the model\n",
    "    model_w2v_train = load(open(\"./pickle_files/train_w2v_model.pkl\",\"rb\"))\n",
    "\n",
    "if not os.path.isfile('./pickle_files/test_w2v_model.pkl'):\n",
    "    tokenized_tweet = test_dataframe['Tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "   \n",
    "    model_w2v_test = gensim.models.Word2Vec(\n",
    "                tokenized_tweet,\n",
    "                size=500, # desired no. of features/independent variables\n",
    "                window=5, # context window size\n",
    "                min_count=2,\n",
    "                sg = 1, # 1 for skip-gram model\n",
    "                hs = 0,\n",
    "                negative = 10, # for negative sampling\n",
    "                workers= 8, # no.of cores\n",
    "                seed = 34) \n",
    "    \n",
    "    # Training the model\n",
    "    model_w2v_test.train(tokenized_tweet, total_examples= len(test_dataframe['Tweet']), epochs=20)\n",
    "    \n",
    "    # Writing the model to a pickle file\n",
    "    dump(model_w2v_test,open(\"./pickle_files/test_w2v_model.pkl\",\"w+b\"))\n",
    "else:\n",
    "    # If there already exists a pickle file I just load the model\n",
    "    model_w2v_test = load(open(\"./pickle_files/test_w2v_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how good was the resulting training **Word2Vec** model of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_train.wv.most_similar(positive = \"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_train.wv.most_similar(positive = 'mcgregor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model is currently well trained,as we observe that the most similar word of these 2 people is their **first name** and the rest of the most similar words of each name are:\n",
    "\n",
    "<ul>\n",
    "    <li>For <b>trump</b> are political terms or the names of other political persons such as bush</li>\n",
    "    <li>For <b>mcgregor</b> are terms that have to do with UFC/MMA rules or things that are directly relevant with that.Furthermore the people's names that appeared are other fighters that probably have to do with McGregor.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tweet Vectorization based on Word2Vec Model</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet vectorization was made as the project description says and we add as extra features, except the average valences of words at each dictionary, the minimum and maximum valence andthe number on positive and negative words of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_TweetVectorize(tweets,w2v_model):\n",
    "    vectors = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        vector_words_num = 0\n",
    "        splitted_tweet = nltk.word_tokenize(tweet)\n",
    "        vector = np.zeros(w2v_model.wv.vector_size)\n",
    "\n",
    "        dict_appeared_words_num = [0,0,0,0]\n",
    "        dict_valence_sum = [0,0,0,0]\n",
    "        curr_multipliers = [1,1,1,1]\n",
    "\n",
    "        positive_words_num = 0\n",
    "        negative_words_num = 0\n",
    "\n",
    "        min_valence = None\n",
    "        max_valence = None\n",
    "\n",
    "        for word in splitted_tweet:\n",
    "            if word in w2v_model.wv.vocab:\n",
    "                vector += w2v_model[word]\n",
    "                vector_words_num += 1\n",
    "            \n",
    "            appears_pos = 0\n",
    "            appears_neg  = 0\n",
    "            for index in range(0,len(dictionaries)):\n",
    "                if word in dictionaries[index].keys():\n",
    "                    dict_appeared_words_num[index] += 1\n",
    "\n",
    "                    if word != 'not':\n",
    "                        dict_valence_sum[index] += curr_multipliers[index]*dictionaries[index][word]\n",
    "                        curr_valence = curr_multipliers[index]*dictionaries[index][word]\n",
    "\n",
    "                        if dictionaries[index][word] >= 0:\n",
    "                            if curr_multipliers[index] == 1:\n",
    "                                appears_pos += 1\n",
    "                            else:\n",
    "                                appears_neg += 1\n",
    "                        else:\n",
    "                            if curr_multipliers[index] == 1:\n",
    "                                appears_neg += 1\n",
    "                            else:\n",
    "                                appears_pos += 1\n",
    "\n",
    "                        if min_valence == None or min_valence > curr_valence:\n",
    "                            min_valence = curr_valence\n",
    "\n",
    "                        if max_valence == None or max_valence < curr_valence:\n",
    "                            max_valence = curr_valence\n",
    "                        \n",
    "                        curr_multipliers[index] = 1\n",
    "                    else:\n",
    "                        dict_valence_sum[index] += dictionaries[index]['not']\n",
    "                        curr_multipliers[index] = -1\n",
    "            \n",
    "            if (appears_pos != 0) or (appears_neg != 0):\n",
    "                if appears_pos >= appears_neg:\n",
    "                    positive_words_num += 1\n",
    "                else:\n",
    "                    negative_words_num += 1\n",
    "                        \n",
    "        for i in range(0,len(dict_appeared_words_num)):\n",
    "            if dict_appeared_words_num[i] == 0:\n",
    "                dict_appeared_words_num[i] = 1\n",
    "        \n",
    "        dict_valence_sum = np.array(dict_valence_sum)\n",
    "        dict_appeared_words_num = np.array(dict_appeared_words_num)\n",
    "\n",
    "        if vector_words_num == 0:\n",
    "            vector = vector\n",
    "        else:\n",
    "            vector = vector / vector_words_num\n",
    "        \n",
    "        vector = np.append(vector,dict_valence_sum / dict_appeared_words_num)\n",
    "        vector = np.append(vector,[positive_words_num,negative_words_num])\n",
    "\n",
    "        if min_valence == None:\n",
    "            min_valence = 0\n",
    "        \n",
    "        if max_valence == None:\n",
    "            max_valence = 0\n",
    "        \n",
    "        vector = np.append(vector,[max_valence,min_valence])\n",
    "\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./pickle_files/w2v_train_vectors.pkl'):\n",
    "    w2v_train_vectors = W2V_TweetVectorize(training_dataframe['Tweet'],model_w2v_train)\n",
    "\n",
    "    dump(w2v_train_vectors,open(\"./pickle_files/w2v_train_vectors.pkl\",\"w+b\"))\n",
    "else:\n",
    "    w2v_train_vectors = load(open(\"./pickle_files/w2v_train_vectors.pkl\",\"rb\"))\n",
    "\n",
    "if not os.path.isfile('./pickle_files/w2v_test_vectors.pkl'):\n",
    "    w2v_test_vectors = W2V_TweetVectorize(test_dataframe['Tweet'],model_w2v_test)\n",
    "\n",
    "    dump(w2v_test_vectors,open(\"./pickle_files/w2v_test_vectors.pkl\",\"w+b\"))\n",
    "else:\n",
    "    w2v_test_vectors = load(open(\"./pickle_files/w2v_test_vectors.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>TSNE plot function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model,words_to_plot):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in list(model.wv.vocab)[0:words_to_plot + 1]:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                xy=(x[i], y[i]),\n",
    "                xytext=(5, 2),\n",
    "                textcoords='offset points',\n",
    "                ha='right',\n",
    "                va='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_num_to_plot = 500\n",
    "tsne_plot(model_w2v_train,words_num_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification Part</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Support Vector Machines Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_Classifier(train_vectors,train_labels,test_vectors,test_labels,vec_mode):\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_vectors, \n",
    "                                        train_labels,\n",
    "                                        random_state=42, test_size=0.2)\n",
    "    \n",
    "    # We need the vec_mode to configure the name of the .pkl files\n",
    "    train_split_model_path = './pickle_files/SVM_train_split_' + vec_mode + '.pkl'\n",
    "    train_full_model_path  = './pickle_files/SVM_train_full_' + vec_mode + '.pkl'\n",
    "\n",
    "    if not os.path.isfile(train_split_model_path):\n",
    "        svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "        svc = svc.fit(xtrain, ytrain) # xtrain:bag of words features for train data, ytrain: train data labels\n",
    "\n",
    "        dump(svc,open(train_split_model_path,'w+b'))\n",
    "    else:\n",
    "        svc = load(open(train_split_model_path,'rb'))\n",
    "    \n",
    "    prediction = svc.predict(xvalid) #predict on the validation set\n",
    "    \n",
    "    # Getting the F1 Score of the cross validation\n",
    "    F1_Score_split = f1_score(yvalid,prediction,average = 'micro') #evaluate on the validation set\n",
    "\n",
    "    if not os.path.isfile(train_full_model_path):\n",
    "        svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "        svc = svc.fit(train_vectors,train_labels)\n",
    "\n",
    "        dump(svc,open(train_full_model_path,'w+b'))\n",
    "    else:\n",
    "        svc = load(open(train_full_model_path,'rb'))\n",
    "    \n",
    "    prediction = svc.predict(test_vectors)\n",
    "    \n",
    "    # Getting the F1 Score of test set prediction\n",
    "    F1_Score = f1_score(test_labels,prediction,average = 'micro')\n",
    "\n",
    "    return (F1_Score_split,F1_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function of the SVM Classifier and getting the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SVM_Classifier(bow_xtrain,training_dataframe['Tag'],\n",
    "                     bow_xtest,test_solutions['Tag'],'bow'))\n",
    "\n",
    "print(SVM_Classifier(tfidf_train,training_dataframe['Tag'],\n",
    "                     tfidf_test,test_solutions['Tag'],'tfidf'))\n",
    "print(SVM_Classifier(w2v_train_vectors,training_dataframe['Tag'],\n",
    "                     w2v_test_vectors,test_solutions['Tag'],'w2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>K-Nearest Neighbors Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Classifier(train_vectors,train_labels,test_vectors,test_labels):\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(train_vectors, \n",
    "                                        train_labels,\n",
    "                                        random_state=42, test_size=0.2)   \n",
    "\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors = 10)\n",
    "\n",
    "    knn_classifier = knn_classifier.fit(xtrain,ytrain)\n",
    "    prediction = knn_classifier.predict(xvalid)\n",
    "\n",
    "    F1_Score_split = f1_score(yvalid,prediction,average = 'micro')\n",
    "\n",
    "    knn_classifier = knn_classifier.fit(train_vectors,train_labels)\n",
    "    prediction = knn_classifier.predict(test_vectors)\n",
    "\n",
    "    F1_Score = f1_score(test_labels,prediction,average = 'micro')\n",
    "\n",
    "    return (F1_Score_split,F1_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function of the KNN Classifier and getting the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KNN_Classifier(bow_xtrain,training_dataframe['Tag'],\n",
    "                     bow_xtest,test_solutions['Tag']))\n",
    "\n",
    "print(KNN_Classifier(tfidf_train,training_dataframe['Tag'],\n",
    "                     tfidf_test,test_solutions['Tag']))\n",
    "\n",
    "print(KNN_Classifier(w2v_train_vectors,training_dataframe['Tag'],\n",
    "                     w2v_test_vectors,test_solutions['Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
